{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImages_hdf5(filename):\n",
    "    '''Reads hdf5 file.\n",
    "       Parameter\n",
    "       ---------\n",
    "       filename : the name of the hdf5 file\n",
    "    '''\n",
    "    file = h5py.File( filename + '.h5', \"r+\") #open the hdf5 file.\n",
    "    \n",
    "    hdf5_images = np.array(file[\"/images\"]).astype(\"uint8\") #read the images as np array\n",
    "    hdf5_labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "    \n",
    "    return hdf5_images, hdf5_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(label_arr):\n",
    "    '''Returns the given MNIST labels from np arrays of integers to np array of one hot labels.\n",
    "       Parameter\n",
    "       ---------\n",
    "       label_arr : np array of MNIST integer labels\n",
    "    '''\n",
    "    total_labels  = label_arr.shape[0] #get the total number of labels\n",
    "    one_hot_label = np.zeros([total_labels, 10]) #10 for num of classes in MNIST\n",
    "    \n",
    "    for i in range(label_arr.shape[0]): #loop through all the labels\n",
    "        \n",
    "        one_hot_label[i][int(label_arr[i])] = 1.0 #the label value will be marked as 1.0 at that specific index\n",
    "        \n",
    "    return one_hot_label #returns the np one-hot label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './60000'\n",
    "\n",
    "images, labels = readImages_hdf5(filename)\n",
    "labels = one_hot_encoder(labels)\n",
    "images = images.reshape(images.shape[0], 784)\n",
    "\n",
    "is_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(inputs, is_training, decay=0.999):\n",
    "\n",
    "\t\tscale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "\t\tbeta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "\n",
    "\n",
    "\n",
    "\t\tpop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "\t\tpop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "\t\tif is_training:\n",
    "\n",
    "\t\t\tbatch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "\n",
    "\t\t\ttrain_mean = tf.assign(pop_mean,pop_mean*decay + batch_mean * (1 - decay))\n",
    "\n",
    "\t\t\ttrain_var = tf.assign(pop_var, pop_var*decay + batch_var * (1 - decay))\n",
    "\n",
    "\t\t\twith tf.control_dependencies([train_mean, train_var]):\n",
    "\t\t\t\treturn tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, 1e-9)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\treturn tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a0d1cbc50668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbatch_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_mean\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_mean\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop_var\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_var\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decay' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def build_graph(is_training = True):\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, 256], stddev=0.1))\n",
    "    B1 = tf.Variable(tf.ones([256]))\n",
    "\n",
    "    fc1 = tf.add(tf.matmul(x, W1), B1)\n",
    "\n",
    "    ####################################\n",
    "    scale = tf.Variable(tf.ones([fc1.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([fc1.get_shape()[-1]]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([fc1.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([fc1.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    fc1_hat = None\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(fc1, [0])\n",
    "\n",
    "        train_mean = tf.assign(pop_mean,pop_mean*decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var, pop_var*decay + batch_var * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            fc1_hat = tf.nn.batch_normalization(fc1, batch_mean, batch_var, beta, scale, 1e-9)\n",
    "\n",
    "    else:\n",
    "        fc1_hat =  tf.nn.batch_normalization(fc1, pop_mean, pop_var, beta, scale, 1e-9)\n",
    "\n",
    "    ####################################\n",
    "\n",
    "    fc1_actv = tf.nn.relu(fc1_hat)\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([256, 100], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.ones([100]))\n",
    "\n",
    "    fc2 =  tf.add(tf.matmul(fc1_actv, W2), B2)\n",
    "\n",
    "    fc2_actv = tf.nn.relu(fc2)\n",
    "\n",
    "    W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.ones([10]))\n",
    "\n",
    "    logits = tf.add(tf.matmul(fc2_actv, W3), B3)\n",
    "\n",
    "    Y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "\tsaver.restore(sess, args.model_save_path + 'model.ckpt')\n",
    "\tprint(\"Model has been loaded !\")\n",
    "\n",
    "except:\n",
    "\n",
    "\tprint(\"Model is not loaded !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    \n",
    "    loss_total = 0\n",
    "    accuracy_total = 0\n",
    "    counter = 0\n",
    "    for i in range(0,60000, 200):\n",
    "        \n",
    "        idx = i + 200\n",
    "        if idx >= 60000 : idx = 59999\n",
    "    \n",
    "        _, loss_val, accuracy_val = sess.run([optimizer, loss, accuracy], feed_dict={\n",
    "            x : images[i:idx],\n",
    "            y : labels[i:idx]\n",
    "        })\n",
    "        \n",
    "        loss_total += loss_val\n",
    "        accuracy_total += accuracy_val\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"Epoch %d, loss %g, accuracy %g\"%(k, loss_total, accuracy_total/counter))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
