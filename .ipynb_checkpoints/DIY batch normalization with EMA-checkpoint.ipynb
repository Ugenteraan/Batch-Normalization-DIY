{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImages_hdf5(filename):\n",
    "    '''Reads hdf5 file.\n",
    "       Parameter\n",
    "       ---------\n",
    "       filename : the name of the hdf5 file\n",
    "    '''\n",
    "    file = h5py.File( filename + '.h5', \"r+\") #open the hdf5 file.\n",
    "    \n",
    "    hdf5_images = np.array(file[\"/images\"]).astype(\"uint8\") #read the images as np array\n",
    "    hdf5_labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "    \n",
    "    return hdf5_images, hdf5_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(label_arr):\n",
    "    '''Returns the given MNIST labels from np arrays of integers to np array of one hot labels.\n",
    "       Parameter\n",
    "       ---------\n",
    "       label_arr : np array of MNIST integer labels\n",
    "    '''\n",
    "    total_labels  = label_arr.shape[0] #get the total number of labels\n",
    "    one_hot_label = np.zeros([total_labels, 10]) #10 for num of classes in MNIST\n",
    "    \n",
    "    for i in range(label_arr.shape[0]): #loop through all the labels\n",
    "        \n",
    "        one_hot_label[i][int(label_arr[i])] = 1.0 #the label value will be marked as 1.0 at that specific index\n",
    "        \n",
    "    return one_hot_label #returns the np one-hot label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './60000'\n",
    "\n",
    "images, labels = readImages_hdf5(filename)\n",
    "labels = one_hot_encoder(labels)\n",
    "images = images.reshape(images.shape[0],28, 28, 1) #reshape into a tensor of rank 4 for CNN filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "class Model():\n",
    "    \n",
    "    \n",
    "    def normalize(self, inputs, batch_mean, batch_var, scale, beta, epsilon=1e-9):\n",
    "        '''\n",
    "        From the paper, to normalize the input, we should minus the input by the mean of the whole batch\n",
    "        and divide it by the square root of its variance. Epsilon is added to avoid division by zero.\n",
    "        The normalized input will then be transformed linearly using two learnable parameters. Scale and beta.\n",
    "        '''\n",
    "        \n",
    "        input_hat = (inputs - batch_mean) / tf.sqrt(batch_var + epsilon)\n",
    "        \n",
    "        normalized = scale*input_hat + beta\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def batch_norm_wrapper(self, inputs, is_training, decay=0.999, is_conv=False):\n",
    "        '''\n",
    "        First we need to initialize the parameters. Scale and beta is used for linear transformation of the \n",
    "        normalized input. \n",
    "        In order for the model to work during testing time, we need to measure the population mean and variance.\n",
    "        Think about it, if you provide only 1 data during testing, there would be no mean nor variance.\n",
    "        Therefore, we use calculate the moving average of the mean and variance of each batch to estimate the\n",
    "        population variance and mean. Here we used an exponential moving average for easier implementation.\n",
    "        IMPORTANT: Note that this implementation only works for a relatively large dataset. The larger the \n",
    "        dataset, the closer the value of the decay to 1 should be.\n",
    "        '''\n",
    "    \n",
    "        scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "        beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "        \n",
    "        #For population mean and variance.\n",
    "        #NOTE: Even though the variable name is same and being called more than one time, since the shape is\n",
    "        #different, tensorflow graph treats them as different separate variables each time the function is\n",
    "        #called with different inputs.\n",
    "        pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "        pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "        #During training, we want to calculate the exponential moving average of the batch mean and batch\n",
    "        #variance to estimate the population mean and variance. \n",
    "        if is_training:\n",
    "\n",
    "            batch_mean, batch_var = None,None\n",
    "            #For conv networks, we have to calculate the mean and variance for three axes since its a 4D input.\n",
    "            if is_conv :\n",
    "                batch_mean, batch_var = tf.nn.moments(inputs, [0,1,2])\n",
    "            else:\n",
    "                batch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "\n",
    "            train_mean = tf.assign(pop_mean,pop_mean*decay + batch_mean * (1 - decay))\n",
    "            train_var = tf.assign(pop_var, pop_var*decay + batch_var * (1 - decay))\n",
    "    \n",
    "            #We need to use this so that train_mean and train_var op will be run first before it returns\n",
    "            #the normalized value back.Note that the train_mean and train_var variable is not part of the \n",
    "            #optimizer's concern.\n",
    "            with tf.control_dependencies([train_mean, train_var]):\n",
    "                return self.normalize(inputs, batch_mean, batch_var, scale, beta, 1e-9)\n",
    "\n",
    "        else:\n",
    "            #During testing, we use the esitmated population mean and variance instead.\n",
    "            return self.normalize(inputs, pop_mean, pop_var, scale, beta, 1e-9)\n",
    "        \n",
    "    \n",
    "    def __init__(self, is_training):\n",
    "    \n",
    "        self.x = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        conv1 = tf.contrib.layers.conv2d(self.x, num_outputs=64, kernel_size=3, stride=1,padding='SAME', activation_fn=None)\n",
    "    \n",
    "        #batch normalization before the activation function but after the linear transformation.\n",
    "        conv1_actv = tf.nn.relu(self.batch_norm_wrapper(conv1, is_training=is_training, is_conv=True))\n",
    "\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1_actv, num_outputs=64, kernel_size=3, stride=2,padding='SAME', activation_fn=None)\n",
    "        \n",
    "        #batch normalization before the activation function but after the linear transformation.\n",
    "        conv2_actv = tf.nn.relu(self.batch_norm_wrapper(conv2, is_training=is_training, is_conv=True))\n",
    "\n",
    "        output_size = 14*14*64\n",
    "        output_layer = tf.reshape(conv2_actv, (-1, output_size))\n",
    "\n",
    "\n",
    "        W2 = tf.Variable(tf.truncated_normal([output_size, 100], stddev=0.1))\n",
    "        B2 = tf.Variable(tf.ones([100]))\n",
    "\n",
    "        fc2 =  tf.add(tf.matmul(output_layer, W2), B2)\n",
    "        \n",
    "        #batch normalization before the activation function but after the linear transformation.\n",
    "        fc2_actv = tf.nn.relu(self.batch_norm_wrapper(fc2, is_training=is_training))\n",
    "\n",
    "        W3 = tf.Variable(tf.truncated_normal([100, 10], stddev=0.1))\n",
    "        B3 = tf.Variable(tf.ones([10]))\n",
    "\n",
    "        self.logits = tf.add(tf.matmul(fc2_actv, W3), B3)\n",
    "\n",
    "        Y_pred = tf.nn.softmax(self.logits)\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.loss)\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Model(is_training=True) #passing in the parameter as True is Important!\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "nasa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
